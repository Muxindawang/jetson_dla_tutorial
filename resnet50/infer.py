#!/usr/bin/env python3
"""TensorRT Inference Script for Jetson Orin (with DLA support)
Supports ResNet-18/50 etc. exported from torchvision.

Usage:
    # æ¨ç†å•å¼ å›¾åƒ
    python3 infer.py model.engine --image cat.jpg
    # éšæœºæ•°æ®æ¨ç†ï¼ˆæ€§èƒ½æµ‹è¯•ï¼‰
    python3 infer.py model.engine --batch_size 8 --num_runs 100
"""

import argparse
import os
import time
import numpy as np

if not hasattr(np, 'bool'):
    np.bool = bool
if not hasattr(np, 'int'):
    np.int = int
if not hasattr(np, 'float'):
    np.float = float

# =============================================
import pycuda.driver as cuda
import pycuda.autoinit  # å¿…é¡»å¯¼å…¥ä»¥åˆå§‹åŒ– CUDA ä¸Šä¸‹æ–‡
import tensorrt as trt

# ImageNet ç±»åˆ«æ ‡ç­¾ï¼ˆç®€åŒ–ç‰ˆï¼Œä»…å‰ 1000 ç±»ï¼‰
try:
    import json
    with open('imagenet_class_index.json', 'r') as f:
        IMAGENET_CLASSES = json.load(f)
except Exception as e:
    print(f"âš ï¸ æœªæ‰¾åˆ° imagenet_class_index.json æˆ–åŠ è½½å¤±è´¥: {e}")
    IMAGENET_CLASSES = [f"class_{i}" for i in range(1000)]


def load_engine(engine_path):
    if not os.path.exists(engine_path):
        raise FileNotFoundError(f"å¼•æ“æ–‡ä»¶ä¸å­˜åœ¨: {engine_path}")
    t0 = time.perf_counter()
    with open(engine_path, "rb") as f, trt.Runtime(trt.Logger(trt.Logger.WARNING)) as runtime:
        engine = runtime.deserialize_cuda_engine(f.read())
        if engine is None:
            raise RuntimeError("ååºåˆ—åŒ–å¼•æ“å¤±è´¥ï¼")
    t1 = time.perf_counter()
    print(f"[â±ï¸] å¼•æ“åŠ è½½è€—æ—¶: {(t1 - t0) * 1000:.2f} ms")
    return engine


def allocate_buffers(engine, batch_size=1):
    inputs, outputs, bindings = [], [], []
    stream = cuda.Stream()

    t0 = time.perf_counter()
    for binding in engine:
        shape = list(engine.get_binding_shape(binding))
        if shape[0] == -1:
            shape[0] = batch_size
        size = trt.volume(shape) * engine.max_batch_size
        dtype = trt.nptype(engine.get_binding_dtype(binding))
        host_mem = cuda.pagelocked_empty(size, dtype)
        device_mem = cuda.mem_alloc(host_mem.nbytes)
        bindings.append(int(device_mem))
        if engine.binding_is_input(binding):
            inputs.append({'host': host_mem, 'device': device_mem, 'shape': tuple(shape)})
        else:
            outputs.append({'host': host_mem, 'device': device_mem, 'shape': tuple(shape)})
    t1 = time.perf_counter()
    print(f"[â±ï¸] ç¼“å†²åŒºåˆ†é…è€—æ—¶: {(t1 - t0) * 1000:.2f} ms")
    return inputs, outputs, bindings, stream


def preprocess_image(image_path, target_size=(224, 224)):
    from PIL import Image
    image = Image.open(image_path).convert('RGB')
    image = image.resize(target_size, Image.LANCZOS)
    img_array = np.array(image, dtype=np.float32) / 255.0
    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)
    std = np.array([0.229, 0.224, 0.225], dtype=np.float32)
    img_array = (img_array - mean) / std
    img_array = np.transpose(img_array, (2, 0, 1))
    return img_array


def run_inference(context, bindings, inputs, outputs, stream, input_data):
    np.copyto(inputs[0]['host'], input_data.ravel())
    cuda.memcpy_htod_async(inputs[0]['device'], inputs[0]['host'], stream)
    context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)
    cuda.memcpy_dtoh_async(outputs[0]['host'], outputs[0]['device'], stream)
    stream.synchronize()
    return outputs[0]['host'].copy()


def main():
    parser = argparse.ArgumentParser(description="TensorRT Inference with DLA support")
    parser.add_argument("engine", help="Path to TensorRT engine file (.engine)")
    parser.add_argument("--image", "-i", help="Path to input image (for single inference)")
    parser.add_argument("--batch_size", type=int, default=1, help="Batch size for random data test")
    parser.add_argument("--num_runs", type=int, default=10, help="Number of runs for performance test")
    parser.add_argument("--topk", type=int, default=5, help="Number of top predictions to show")
    args = parser.parse_args()

    total_start = time.perf_counter()

    # åŠ è½½å¼•æ“
    engine = load_engine(args.engine)
    context = engine.create_execution_context()
    if not context:
        raise RuntimeError("æ— æ³•åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡")

    # è·å–è¾“å…¥ shape
    input_shape = engine.get_binding_shape(0)
    if input_shape[0] == -1:
        input_shape = (args.batch_size,) + input_shape[1:]
    _, channels, height, width = input_shape
    print(f"âœ… æ£€æµ‹åˆ°æ¨¡å‹è¾“å…¥ shape: {input_shape}")

    # åˆ†é…ç¼“å†²åŒº
    inputs, outputs, bindings, stream = allocate_buffers(engine, batch_size=args.batch_size)

    # å‡†å¤‡è¾“å…¥æ•°æ®
    prep_start = time.perf_counter()
    if args.image:
        if args.batch_size != 1:
            print("âš ï¸ æŒ‡å®šäº†å›¾åƒï¼Œå¼ºåˆ¶ batch_size=1")
            args.batch_size = 1
        input_data = preprocess_image(args.image, (height, width))
        input_data = np.expand_dims(input_data, axis=0)
        print(f"ğŸ–¼ï¸ åŠ è½½å›¾åƒ: {args.image}")
    else:
        print(f"ğŸ² ä½¿ç”¨éšæœºæ•°æ® (batch_size={args.batch_size})")
        input_shape = (args.batch_size, channels, height, width)
        input_data = np.random.randn(*input_shape).astype(np.float32)
    prep_end = time.perf_counter()
    print(f"[â±ï¸] è¾“å…¥å‡†å¤‡è€—æ—¶: {(prep_end - prep_start) * 1000:.2f} ms")

    # é¢„çƒ­
    print("ğŸ”¥ é¢„çƒ­ä¸­...")
    warmup_start = time.perf_counter()
    for _ in range(50):
        run_inference(context, bindings, inputs, outputs, stream, input_data)
    warmup_end = time.perf_counter()
    print(f"[â±ï¸] é¢„çƒ­è€—æ—¶ (5 æ¬¡): {(warmup_end - warmup_start) * 1000:.2f} ms")

    # æ­£å¼æ¨ç†
    print(f"ğŸš€ å¼€å§‹æ­£å¼æ¨ç† ({args.num_runs} æ¬¡)...")
    infer_start = time.perf_counter()
    for _ in range(args.num_runs):
        output = run_inference(context, bindings, inputs, outputs, stream, input_data)
    infer_end = time.perf_counter()

    total_end = time.perf_counter()

    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    total_time_ms = (total_end - total_start) * 1000
    infer_time_ms = (infer_end - infer_start) * 1000
    avg_latency_ms = infer_time_ms / args.num_runs
    throughput = (args.batch_size * args.num_runs) / (infer_time_ms / 1000)  # imgs/sec

    # è¾“å‡ºç»“æœ
    if args.image:
        probs = output.reshape(-1)
        top_indices = np.argsort(probs)[-args.topk:][::-1]
        print(f"\nğŸ¯ Top-{args.topk} é¢„æµ‹ç»“æœ:")
        for i, idx in enumerate(top_indices):
            class_name = IMAGENET_CLASSES.get(str(idx), IMAGENET_CLASSES[idx]) if isinstance(IMAGENET_CLASSES, dict) else IMAGENET_CLASSES[idx]
            print(f" {i+1}. {class_name} (prob={probs[idx]:.4f})")
    else:
        print(f"\nğŸ“Š æ€§èƒ½ç»Ÿè®¡ (batch_size={args.batch_size}, runs={args.num_runs}):")
        print(f"   å¹³å‡å»¶è¿Ÿ: {avg_latency_ms:.2f} ms")
        print(f"   ååé‡:   {throughput:.1f} images/sec")

    # æ‰“å°å®Œæ•´è€—æ—¶æ‘˜è¦
    print("\n" + "="*50)
    print("[â±ï¸] è€—æ—¶æ±‡æ€»:")
    print(f"   å¼•æ“åŠ è½½:      å·²åœ¨ load_engine ä¸­æ‰“å°")
    print(f"   ç¼“å†²åŒºåˆ†é…:    å·²åœ¨ allocate_buffers ä¸­æ‰“å°")
    print(f"   è¾“å…¥å‡†å¤‡:      {(prep_end - prep_start) * 1000:.2f} ms")
    print(f"   é¢„çƒ­ (5 æ¬¡):   {(warmup_end - warmup_start) * 1000:.2f} ms")
    print(f"   æ­£å¼æ¨ç† ({args.num_runs} æ¬¡): {infer_time_ms:.2f} ms")
    print(f"   â€”â€” å¹³å‡æ¯æ¬¡:   {avg_latency_ms:.2f} ms")
    print(f"   æ€»è€—æ—¶ (ç«¯åˆ°ç«¯): {total_time_ms:.2f} ms")
    print("="*50)

    print("\nâœ… æ¨ç†å®Œæˆï¼è¯·é…åˆ `sudo tegrastats` ç¡®è®¤ DLA æ˜¯å¦å·¥ä½œã€‚")


if __name__ == "__main__":
    main()